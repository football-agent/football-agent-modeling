library(tidyverse)
library(tidymodels)

data = read.csv('df.csv', encoding = 'utf8', stringsAsFactors=FALSE)
# df_df = data[data$DF == 1,]
# data <- data %>% 
#   mutate(
#     elite_team = case_when(
#       squad %in% c("Real Madrid CF", "FC Barcelona") ~ 1,
#       !squad %in% c("Real Madrid CF", "FC Barcelona") ~ 0
#     ))






shuffled_data= data[sample(1:nrow(data)), ]
df <- shuffled_data[, -which(names(shuffled_data) %in% c("X", "season", "player", "squad"))]
df <- df %>%
  relocate(value_eur)

###################################################################################################################
# 
# for (col in colnames(df)) {
#   if (col %in% c("id", "season", "squad", "league", "player", "squad", "contract_signing", "elite_team")){
#     print("skip")
#   }else if(col %in% c("xg_net", "npxg_net", "xa_net")){
#     print("skip because log 0")
#   }else if(col %in% c("plus_minus_per90", "FW", "DF", "GK", "MF", "wc_cards_red", 
#                       "xg_plus_minus_wowy", "xg_plus_minus_per90", "xg_plus_minus", 
#                       "plus_minus", "plus_minus_wowy"
#                       )){
#     print("skip because not significant")
#   } else{
#   print(col)
#   simple.fit = glm(formula = as.formula(paste0("value_eur~",col, "")), 
#                    data = data, family = Gamma("log"))
#   print(summary(simple.fit))
# }
# # plot(data[col],
# #      log(data$value_eur)
# # )
# # abline(model)
# }

not_signif_features = c()
for (col in colnames(df)) {
  if (col %in% c("id", "season", "squad", "league", "player", "squad", "contract_signing", "elite_team")){
    print("skip")
  }else if(col %in% c("xg_net", "npxg_net", "xa_net")){
    print("skip because log 0")
  }else{
    simple.fit = glm(formula = as.formula(paste0("value_eur~",col, "")), 
                     data = data, family = Gamma("log"))
    d <- data.frame(summary(simple.fit)$coef[summary(simple.fit)$coef[,4] <= .05, 4])[2,1]
    if (is.na(d) | d > 0.01){
      not_signif_features <- c(not_signif_features, col)
    }else{
      print(summary(simple.fit))
    }
    
    
  }
}

not_signif_features <- not_signif_features[not_signif_features != "value_eur"]
df <- df[, -which(names(df) %in% not_signif_features)]

# indx <- sapply(df, is.factor)
# df[indx] <- lapply(df[indx], function(x) as.numeric(as.character(x)))
# 
# df2 <- do.call(data.frame,                      # Replace Inf in data by NA
#                    lapply(df,
#                           function(x) replace(x, is.infinite(x), NA)))
# df2 <- do.call(data.frame,                      # Replace Inf in data by NA
#               lapply(df2,
#                      function(x) replace(x, is.nan(x), NA)))
# 
# df2 <- replace_na(df2)

basic.fit = glm(value_eur ~ ., 
                data = df, family = gaussian("log"))

print(summary(basic.fit))


################################################################################
get_aic <- function(model){
  return(summary(model)$aic)
}

transform_log = list()


for (col in colnames(df)) {
  if (col == "value_eur"){next}
  if (col == "rich_team"){next}
  if (col == "best_age"){next}
  if (col == "players_dribbled_past"){next}
  if (col == "dribbles_completed_diff"){next}
  if (col == "npxg_net"){next}
  if (col == "xg_net"){next}
  if (col == "xa_net"){next}
  
  
  log.aic = Inf
  sqrt.aic = Inf
  simple.aic = Inf
  
  tryCatch({
    sqrt.fit = glm(formula = as.formula(paste0("value_eur~sqrt(",col, ")")), 
                   data = df, family = gaussian("log"))
    sqrt.aic = get_aic(sqrt.fit)
    
  }, error=function(e){
    print(col)
  })
  tryCatch({
    log.fit = glm(formula = as.formula(paste0("value_eur~log(abs(",col, ") + 1)")), 
                  data = df, family = gaussian("log"))
    log.aic = get_aic(log.fit)
    
  }, error=function(e){
  })
  tryCatch({
    simple.fit = glm(formula = as.formula(paste0("value_eur~",col, "")), 
                     data = df, family = gaussian("log"))
    simple.aic = get_aic(simple.fit)
    
  }, error=function(e){
    # simple.aic = Inf
  })
  
  
  
  aics <- list(
    sqrt = sqrt.aic,
    log = log.aic,
    simple = simple.aic
  )
  
  name = names(which.min(unlist(aics)))  
  if (name == "log"){
    df[col] = log(df[col]+1)
    transform_log[[col]] = "log"  
  }else if(name == "sqrt"){
    df[col] = sqrt(df[col])
    transform_log[[col]] = "sqrt"  
    
  }else if(name == "simple"){
    transform_log[[col]] = "simple"  
  }
}
df <- replace_na(df)
transformed.fit = glm(value_eur ~ ., 
                      data = df, family = gaussian("log"))

print(summary(transformed.fit))





# 
# print(names(df))
# 
# 
# # plot((df$), log(data$value_eur))
# 
# plot(sqrt(data$goals), log(data$value_eur))
# 
# hist(log(data$goals))
# 
# model <- glm(value_eur ~ best_age + I((pass_targets)) + sqrt(gca_passes_live) + I(sqrt(data$minutes)) 
#              , 
#              data = data, family = Gamma("log"))
# summary(model)
# 
# 
# model <- glm(value_eur ~ ., 
#              data = data, family = Gamma("log"))
# summary(model)


library(tidyverse)
library(tidymodels)
library(xgboost)
library(caret)
library(DiagrammeR)



###################################################################################################################
# sample <- sample.split(df$X, SplitRatio = 0.8)
# train_data <- subset(df, sample == TRUE)
# test_data <- subset(df, sample == FALSE)

df$value_eur = log(df$value_eur)
d <- data.frame(df)
m <- as.matrix(d)

set.seed(14)
indices <- sample(1:nrow(df), size = 0.8 * nrow(df))
train <- m[indices,]
test <- m[-indices,]

####################################################################################


####################################################################################
hyper_grid <- expand.grid(max_depth = seq(5,5,2),
                          eta = c(0.05, 0.025, 0.075),
                          gamma = seq(.3, .7, .2),
                          subsample = seq(.3, .7, .2),
                          colsample_bytree = seq(0.3, 0.7, 0.2)
                          # lambda = c(1e-2, 1e-4, 1e-6, 1e-7)
                          
)
# hyper_grid <- expand.grid(max_depth = seq(1,5,2),
#                           eta = seq(.1, .7, .2),
#                           gamma = seq(0.9, 0.9, 0.2),
#                           subsample = seq(0.5, 0.9, 0.2),
#                           colsample_bytree = seq(0.8, 0.95, 0.05)
# )
xgb_train_rmse <- NULL
xgb_test_rmse <- NULL
best_iterations <- NULL

for (j in 1:nrow(hyper_grid)) {
  set.seed(1314)
  
  m_xgb_untuned <- xgb.cv(
    booster = "gbtree",
    # metrics = "rmse",
    data = train[, 2:length(df)],
    label = train[, 1],
    nrounds = 1000,
    objective = "reg:squarederror",
    early_stopping_rounds = 30,
    nfold = 10,
    max_depth = hyper_grid$max_depth[j],
    eta = hyper_grid$eta[j],
    gamma = hyper_grid$gamma[j],
    subsample = hyper_grid$subsample[j],
    colsample_bytree = hyper_grid$colsample_bytree[j],
    print_every_n = 50,
    nthread = 8
    
  )
  
  # xgb_train_rmse[j] <- m_xgb_untuned$evaluation_log$train_gamma_nloglik_mean[m_xgb_untuned$best_iteration]
  # xgb_test_rmse[j] <- m_xgb_untuned$evaluation_log$test_gamma_nloglik_mean[m_xgb_untuned$best_iteration]
  xgb_train_rmse[j] <- m_xgb_untuned$evaluation_log$train_rmse_mean[m_xgb_untuned$best_iteration]
  xgb_test_rmse[j] <- m_xgb_untuned$evaluation_log$test_rmse_mean[m_xgb_untuned$best_iteration]
  # xgb_train_rmse[j] <- m_xgb_untuned$evaluation_log$train_mae_mean[m_xgb_untuned$best_iteration]
  # xgb_test_rmse[j] <- m_xgb_untuned$evaluation_log$test_mae_mean[m_xgb_untuned$best_iteration]
  
  best_iterations[j] <- m_xgb_untuned$best_iteration
  
  cat(j, "\n")
}

#ideal hyperparamters
hyper_grid[which.min(xgb_test_rmse), ]
best_iterations[which.min(xgb_test_rmse)]

####################################################################################
m1_xgb <-
  xgboost(
    data = train[, 2:length(df)],
    label = train[, 1],
    nrounds = best_iterations[which.min(xgb_test_rmse)],
    objective = "reg:squarederror",
    early_stopping_rounds = 10,
    max_depth =  hyper_grid[which.min(xgb_test_rmse), ]$max_depth,
    eta = hyper_grid[which.min(xgb_test_rmse), ]$eta,
    gamma = hyper_grid[which.min(xgb_test_rmse), ]$gamma,
    subsample = hyper_grid[which.min(xgb_test_rmse), ]$subsample,
    colsample_bytree = hyper_grid[which.min(xgb_test_rmse), ]$colsample_bytree,
    print_every_n = 10, set.seed(1314)
  )


pred_xgb <- predict(m1_xgb, test[, 2:length(df)])
yhat <- pred_xgb
y <- test[, 1]

r <- exp(y) - exp(yhat)
plot(r, ylab = "residuals")

importance_matrix <- xgb.importance(model = m1_xgb)
xgb.plot.importance(importance_matrix[1:10,], xlab = "Feature Importance")

postResample(exp(yhat), exp(y))
plot(exp(y),
     exp(yhat),
     xlab = "actual",
     ylab = "predicted")
abline(lm(exp(yhat) ~ exp(y)))
abline(coef = c(0,1))
