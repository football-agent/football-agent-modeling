for (j in 1:nrow(hyper_grid)) {
set.seed(1314)
m_xgb_untuned <- xgb.cv(
data = train[, 2:length(df)],
label = train[, 1],
nrounds = 1000,
objective = "reg:gamma",
early_stopping_rounds = 30,
nfold = 5,
max_depth = hyper_grid$max_depth[j],
eta = hyper_grid$eta[j],
gamma = hyper_grid$gamma[j],
subsample = hyper_grid$subsample[j],
colsample_bytree = hyper_grid$colsample_bytree[j],
print_every_n = 50,
nthread = 8
)
xgb_train_rmse[j] <- m_xgb_untuned$evaluation_log$train_gamma_nloglik_mean[m_xgb_untuned$best_iteration]
xgb_test_rmse[j] <- m_xgb_untuned$evaluation_log$test_gamma_nloglik_mean[m_xgb_untuned$best_iteration]
# xgb_train_rmse[j] <- m_xgb_untuned$evaluation_log$train_rmse_mean[m_xgb_untuned$best_iteration]
# xgb_test_rmse[j] <- m_xgb_untuned$evaluation_log$test_rmse_mean[m_xgb_untuned$best_iteration]
best_iterations[j] <- m_xgb_untuned$best_iteration
cat(j, "\n")
}
m1_xgb <-
xgboost(
data = train[, 2:length(df)],
label = train[, 1],
nrounds = best_iteration,
objective = "reg:gamma",
early_stopping_rounds = NULL,
max_depth =  hyper_grid[which.min(xgb_test_rmse), ]$max_depth,
eta = hyper_grid[which.min(xgb_test_rmse), ]$eta,
gamma = hyper_grid[which.min(xgb_test_rmse), ]$gamma,
subsample = hyper_grid[which.min(xgb_test_rmse), ]$subsample,
colsample_bytree = hyper_grid[which.min(xgb_test_rmse), ]$colsample_bytree,
print_every_n = 10
)
pred_xgb <- predict(m1_xgb, test[, 2:length(df)])
yhat <- pred_xgb
y <- test[, 1]
postResample(yhat, y)
hyper_grid[which.min(xgb_test_rmse), ]
best_iteration <- best_iterations[which.min(xgb_test_rmse)]
m1_xgb <-
xgboost(
data = train[, 2:length(df)],
label = train[, 1],
nrounds = best_iteration,
objective = "reg:gamma",
early_stopping_rounds = NULL,
max_depth =  hyper_grid[which.min(xgb_test_rmse), ]$max_depth,
eta = hyper_grid[which.min(xgb_test_rmse), ]$eta,
gamma = hyper_grid[which.min(xgb_test_rmse), ]$gamma,
subsample = hyper_grid[which.min(xgb_test_rmse), ]$subsample,
colsample_bytree = hyper_grid[which.min(xgb_test_rmse), ]$colsample_bytree,
print_every_n = 10
)
pred_xgb <- predict(m1_xgb, test[, 2:length(df)])
yhat <- pred_xgb
y <- test[, 1]
postResample(yhat, y)
plot(y,
yhat,
xlab = "actual",
ylab = "predicted")
abline(lm(yhat ~ y))
abline(coef = c(0,1))
source('~/football-agent-services/football_stats/FW.R')
postResample(yhat, y)
r <- y - yhat
plot(r, ylab = "residuals")
r <- y - yhat
plot(r, ylab = "residuals")
plot(y,
yhat,
xlab = "actual",
ylab = "predicted")
abline(lm(yhat ~ y))
abline(coef = c(0,1))
importance_matrix <- xgb.importance(model = m1_xgb)
xgb.plot.importance(importance_matrix[1:10,], xlab = "Feature Importance")
m1_xgb <-
xgboost(
data = train[, 2:length(df)],
label = train[, 1],
nrounds = best_iteration,
objective = "reg:gamma",
early_stopping_rounds = NULL,
max_depth =  hyper_grid[which.min(xgb_test_rmse), ]$max_depth,
eta = hyper_grid[which.min(xgb_test_rmse), ]$eta,
gamma = hyper_grid[which.min(xgb_test_rmse), ]$gamma,
subsample = hyper_grid[which.min(xgb_test_rmse), ]$subsample,
colsample_bytree = hyper_grid[which.min(xgb_test_rmse), ]$colsample_bytree,
print_every_n = 10, set.seed(1314)
)
pred_xgb <- predict(m1_xgb, test[, 2:length(df)])
yhat <- pred_xgb
y <- test[, 1]
postResample(yhat, y)
postResample(yhat, y)
pred_xgb <- predict(m1_xgb, test[, 2:length(df)])
yhat <- pred_xgb
y <- test[, 1]
postResample(yhat, y)
r <- y - yhat
plot(r, ylab = "residuals")
plot(y,
yhat,
xlab = "actual",
ylab = "predicted")
abline(lm(yhat ~ y))
abline(coef = c(0,1))
hyper_grid <- expand.grid(max_depth = seq(1,5,2),
eta = seq(.1, .9, .4),
gamma = seq(.1, .9, .4),
subsample = seq(.1, .9, .4),
colsample_bytree = seq(0.8, 0.95, 0.05)
)
# hyper_grid <- expand.grid(max_depth = seq(1,5,2),
#                           eta = seq(.1, .7, .2),
#                           gamma = seq(0.9, 0.9, 0.2),
#                           subsample = seq(0.5, 0.9, 0.2),
#                           colsample_bytree = seq(0.8, 0.95, 0.05)
# )
xgb_train_rmse <- NULL
xgb_test_rmse <- NULL
best_iterations <- NULL
for (j in 1:nrow(hyper_grid)) {
set.seed(1314)
m_xgb_untuned <- xgb.cv(
data = train[, 2:length(df)],
label = train[, 1],
nrounds = 1000,
objective = "reg:gamma",
early_stopping_rounds = 30,
nfold = 5,
max_depth = hyper_grid$max_depth[j],
eta = hyper_grid$eta[j],
gamma = hyper_grid$gamma[j],
subsample = hyper_grid$subsample[j],
colsample_bytree = hyper_grid$colsample_bytree[j],
print_every_n = 50,
nthread = 8
)
xgb_train_rmse[j] <- m_xgb_untuned$evaluation_log$train_gamma_nloglik_mean[m_xgb_untuned$best_iteration]
xgb_test_rmse[j] <- m_xgb_untuned$evaluation_log$test_gamma_nloglik_mean[m_xgb_untuned$best_iteration]
# xgb_train_rmse[j] <- m_xgb_untuned$evaluation_log$train_rmse_mean[m_xgb_untuned$best_iteration]
# xgb_test_rmse[j] <- m_xgb_untuned$evaluation_log$test_rmse_mean[m_xgb_untuned$best_iteration]
best_iterations[j] <- m_xgb_untuned$best_iteration
cat(j, "\n")
}
m1_xgb <-
xgboost(
data = train[, 2:length(df)],
label = train[, 1],
nrounds = best_iteration,
objective = "reg:gamma",
early_stopping_rounds = NULL,
max_depth =  hyper_grid[which.min(xgb_test_rmse), ]$max_depth,
eta = hyper_grid[which.min(xgb_test_rmse), ]$eta,
gamma = hyper_grid[which.min(xgb_test_rmse), ]$gamma,
subsample = hyper_grid[which.min(xgb_test_rmse), ]$subsample,
colsample_bytree = hyper_grid[which.min(xgb_test_rmse), ]$colsample_bytree,
print_every_n = 10, set.seed(1314)
)
pred_xgb <- predict(m1_xgb, test[, 2:length(df)])
yhat <- pred_xgb
y <- test[, 1]
postResample(yhat, y)
r <- y - yhat
plot(r, ylab = "residuals")
plot(y,
yhat,
xlab = "actual",
ylab = "predicted")
abline(lm(yhat ~ y))
abline(coef = c(0,1))
#plot first 3 trees of model
# xgb.plot.tree(model = m1_xgb, trees = 0:2)
importance_matrix <- xgb.importance(model = m1_xgb)
xgb.plot.importance(importance_matrix[1:10,], xlab = "Feature Importance")
source('~/football-agent-services/football_stats/FW.R')
best_iteration <- best_iterations[which.min(xgb_test_rmse)]
m1_xgb <-
xgboost(
data = train[, 2:length(df)],
label = train[, 1],
nrounds = best_iteration,
objective = "reg:squarederror",
early_stopping_rounds = NULL,
max_depth =  hyper_grid[which.min(xgb_test_rmse), ]$max_depth,
eta = hyper_grid[which.min(xgb_test_rmse), ]$eta,
gamma = hyper_grid[which.min(xgb_test_rmse), ]$gamma,
subsample = hyper_grid[which.min(xgb_test_rmse), ]$subsample,
colsample_bytree = hyper_grid[which.min(xgb_test_rmse), ]$colsample_bytree,
print_every_n = 10, set.seed(1314)
)
pred_xgb <- predict(m1_xgb, test[, 2:length(df)])
yhat <- pred_xgb
y <- test[, 1]
postResample(yhat, y)
r <- y - yhat
plot(r, ylab = "residuals")
plot(y,
yhat,
xlab = "actual",
ylab = "predicted")
abline(lm(yhat ~ y))
abline(coef = c(0,1))
#plot first 3 trees of model
# xgb.plot.tree(model = m1_xgb, trees = 0:2)
importance_matrix <- xgb.importance(model = m1_xgb)
xgb.plot.importance(importance_matrix[1:10,], xlab = "Feature Importance")
postResample(exp(yhat), exp(y))
plot(exp(y),
exp(yhat),
xlab = "actual",
ylab = "predicted")
abline(lm(exp(yhat) ~ exp(y)))
m1_xgb <-
xgboost(
data = train[, 2:length(df)],
label = train[, 1],
nrounds = best_iteration,
objective = "reg:gamma",
early_stopping_rounds = NULL,
max_depth =  hyper_grid[which.min(xgb_test_rmse), ]$max_depth,
eta = hyper_grid[which.min(xgb_test_rmse), ]$eta,
gamma = hyper_grid[which.min(xgb_test_rmse), ]$gamma,
subsample = hyper_grid[which.min(xgb_test_rmse), ]$subsample,
colsample_bytree = hyper_grid[which.min(xgb_test_rmse), ]$colsample_bytree,
print_every_n = 10, set.seed(1314)
)
pred_xgb <- predict(m1_xgb, test[, 2:length(df)])
yhat <- pred_xgb
y <- test[, 1]
postResample(yhat, y)
r <- y - yhat
plot(r, ylab = "residuals")
plot(y,
yhat,
xlab = "actual",
ylab = "predicted")
abline(lm(yhat ~ y))
abline(coef = c(0,1))
#plot first 3 trees of model
# xgb.plot.tree(model = m1_xgb, trees = 0:2)
importance_matrix <- xgb.importance(model = m1_xgb)
xgb.plot.importance(importance_matrix[1:10,], xlab = "Feature Importance")
postResample(exp(yhat), exp(y))
plot(exp(y),
exp(yhat),
xlab = "actual",
ylab = "predicted")
abline(lm(exp(yhat) ~ exp(y)))
importance_matrix <- xgb.importance(model = m1_xgb)
xgb.plot.importance(importance_matrix[1:10,], xlab = "Feature Importance")
postResample(exp(yhat), exp(y))
plot(exp(y),
exp(yhat),
xlab = "actual",
ylab = "predicted")
abline(lm(exp(yhat) ~ exp(y)))
source('~/football-agent-services/football_stats/FW.R')
m1_xgb <-
xgboost(
data = train[, 2:length(df)],
label = train[, 1],
nrounds = best_iteration,
objective = "reg:gamma",
early_stopping_rounds = NULL,
max_depth =  hyper_grid[which.min(xgb_test_rmse), ]$max_depth,
eta = hyper_grid[which.min(xgb_test_rmse), ]$eta,
gamma = hyper_grid[which.min(xgb_test_rmse), ]$gamma,
subsample = hyper_grid[which.min(xgb_test_rmse), ]$subsample,
colsample_bytree = hyper_grid[which.min(xgb_test_rmse), ]$colsample_bytree,
print_every_n = 10, set.seed(1314)
)
pred_xgb <- predict(m1_xgb, test[, 2:length(df)])
yhat <- pred_xgb
y <- test[, 1]
postResample(yhat, y)
r <- y - yhat
plot(r, ylab = "residuals")
plot(y,
yhat,
xlab = "actual",
ylab = "predicted")
abline(lm(yhat ~ y))
abline(coef = c(0,1))
#plot first 3 trees of model
# xgb.plot.tree(model = m1_xgb, trees = 0:2)
importance_matrix <- xgb.importance(model = m1_xgb)
xgb.plot.importance(importance_matrix[1:10,], xlab = "Feature Importance")
postResample(exp(yhat), exp(y))
plot(exp(y),
exp(yhat),
xlab = "actual",
ylab = "predicted")
abline(lm(exp(yhat) ~ exp(y)))
best_iteration <- best_iterations[which.min(xgb_test_rmse)]
m1_xgb <-
xgboost(
data = train[, 2:length(df)],
label = train[, 1],
nrounds = best_iteration,
objective = "reg:gamma",
early_stopping_rounds = NULL,
max_depth =  hyper_grid[which.min(xgb_test_rmse), ]$max_depth,
eta = hyper_grid[which.min(xgb_test_rmse), ]$eta,
gamma = hyper_grid[which.min(xgb_test_rmse), ]$gamma,
subsample = hyper_grid[which.min(xgb_test_rmse), ]$subsample,
colsample_bytree = hyper_grid[which.min(xgb_test_rmse), ]$colsample_bytree,
print_every_n = 10, set.seed(1314)
)
pred_xgb <- predict(m1_xgb, test[, 2:length(df)])
yhat <- pred_xgb
y <- test[, 1]
postResample(yhat, y)
r <- y - yhat
plot(r, ylab = "residuals")
plot(y,
yhat,
xlab = "actual",
ylab = "predicted")
abline(lm(yhat ~ y))
abline(coef = c(0,1))
#plot first 3 trees of model
# xgb.plot.tree(model = m1_xgb, trees = 0:2)
importance_matrix <- xgb.importance(model = m1_xgb)
xgb.plot.importance(importance_matrix[1:10,], xlab = "Feature Importance")
postResample(exp(yhat), exp(y))
plot(exp(y),
exp(yhat),
xlab = "actual",
ylab = "predicted")
abline(lm(exp(yhat) ~ exp(y)))
importance_matrix <- xgb.importance(model = m1_xgb)
xgb.plot.importance(importance_matrix[1:10,], xlab = "Feature Importance")
source('~/football-agent-services/football_stats/FW.R')
source('~/football-agent-services/football_stats/FW.R')
postResample(exp(yhat), exp(y))
plot(exp(y),
exp(yhat),
xlab = "actual",
ylab = "predicted")
abline(lm(exp(yhat) ~ exp(y)))
abline(coef = c(0,1))
importance_matrix <- xgb.importance(model = m1_xgb)
xgb.plot.importance(importance_matrix[1:10,], xlab = "Feature Importance")
source('~/football-agent-services/football_stats/FW.R')
source('~/football-agent-services/football_stats/FW.R')
m1_xgb <-
xgboost(
data = train[, 2:length(df)],
label = train[, 1],
nrounds = best_iteration,
objective = "reg:gamma",
early_stopping_rounds = NULL,
max_depth =  hyper_grid[which.min(xgb_test_rmse), ]$max_depth,
eta = hyper_grid[which.min(xgb_test_rmse), ]$eta,
gamma = hyper_grid[which.min(xgb_test_rmse), ]$gamma,
subsample = hyper_grid[which.min(xgb_test_rmse), ]$subsample,
colsample_bytree = hyper_grid[which.min(xgb_test_rmse), ]$colsample_bytree,
print_every_n = 10, set.seed(1314)
)
pred_xgb <- predict(m1_xgb, test[, 2:length(df)])
yhat <- pred_xgb
y <- test[, 1]
postResample(yhat, y)
r <- y - yhat
plot(r, ylab = "residuals")
plot(y,
yhat,
xlab = "actual",
ylab = "predicted")
abline(lm(yhat ~ y))
abline(coef = c(0,1))
#plot first 3 trees of model
# xgb.plot.tree(model = m1_xgb, trees = 0:2)
importance_matrix <- xgb.importance(model = m1_xgb)
xgb.plot.importance(importance_matrix[1:10,], xlab = "Feature Importance")
postResample(exp(yhat), exp(y))
plot(exp(y),
exp(yhat),
xlab = "actual",
ylab = "predicted")
abline(lm(exp(yhat) ~ exp(y)))
abline(coef = c(0,1))
m1_xgb <-
xgboost(
data = train[, 2:length(df)],
label = train[, 1],
nrounds = best_iteration,
objective = "reg:squarederror",
early_stopping_rounds = NULL,
max_depth =  hyper_grid[which.min(xgb_test_rmse), ]$max_depth,
eta = hyper_grid[which.min(xgb_test_rmse), ]$eta,
gamma = hyper_grid[which.min(xgb_test_rmse), ]$gamma,
subsample = hyper_grid[which.min(xgb_test_rmse), ]$subsample,
colsample_bytree = hyper_grid[which.min(xgb_test_rmse), ]$colsample_bytree,
print_every_n = 10, set.seed(1314)
)
pred_xgb <- predict(m1_xgb, test[, 2:length(df)])
yhat <- pred_xgb
y <- test[, 1]
postResample(yhat, y)
r <- y - yhat
plot(r, ylab = "residuals")
plot(y,
yhat,
xlab = "actual",
ylab = "predicted")
abline(lm(yhat ~ y))
abline(coef = c(0,1))
#plot first 3 trees of model
# xgb.plot.tree(model = m1_xgb, trees = 0:2)
importance_matrix <- xgb.importance(model = m1_xgb)
xgb.plot.importance(importance_matrix[1:10,], xlab = "Feature Importance")
postResample(exp(yhat), exp(y))
plot(exp(y),
exp(yhat),
xlab = "actual",
ylab = "predicted")
abline(lm(exp(yhat) ~ exp(y)))
abline(coef = c(0,1))
m1_xgb <-
xgboost(
data = train[, 2:length(df)],
label = train[, 1],
nrounds = best_iteration, + 100
objective = "reg:squarederror",
early_stopping_rounds = NULL,
max_depth =  hyper_grid[which.min(xgb_test_rmse), ]$max_depth,
eta = hyper_grid[which.min(xgb_test_rmse), ]$eta,
gamma = hyper_grid[which.min(xgb_test_rmse), ]$gamma,
subsample = hyper_grid[which.min(xgb_test_rmse), ]$subsample,
colsample_bytree = hyper_grid[which.min(xgb_test_rmse), ]$colsample_bytree,
print_every_n = 10, set.seed(1314)
)
pred_xgb <- predict(m1_xgb, test[, 2:length(df)])
yhat <- pred_xgb
y <- test[, 1]
postResample(yhat, y)
r <- y - yhat
plot(r, ylab = "residuals")
plot(y,
yhat,
xlab = "actual",
ylab = "predicted")
abline(lm(yhat ~ y))
abline(coef = c(0,1))
#plot first 3 trees of model
# xgb.plot.tree(model = m1_xgb, trees = 0:2)
importance_matrix <- xgb.importance(model = m1_xgb)
xgb.plot.importance(importance_matrix[1:10,], xlab = "Feature Importance")
postResample(exp(yhat), exp(y))
plot(exp(y),
exp(yhat),
xlab = "actual",
ylab = "predicted")
abline(lm(exp(yhat) ~ exp(y)))
abline(coef = c(0,1))
m1_xgb <-
xgboost(
data = train[, 2:length(df)],
label = train[, 1],
nrounds = best_iteration + 100,
objective = "reg:squarederror",
early_stopping_rounds = NULL,
max_depth =  hyper_grid[which.min(xgb_test_rmse), ]$max_depth,
eta = hyper_grid[which.min(xgb_test_rmse), ]$eta,
gamma = hyper_grid[which.min(xgb_test_rmse), ]$gamma,
subsample = hyper_grid[which.min(xgb_test_rmse), ]$subsample,
colsample_bytree = hyper_grid[which.min(xgb_test_rmse), ]$colsample_bytree,
print_every_n = 10, set.seed(1314)
)
pred_xgb <- predict(m1_xgb, test[, 2:length(df)])
yhat <- pred_xgb
y <- test[, 1]
postResample(yhat, y)
r <- y - yhat
plot(r, ylab = "residuals")
plot(y,
yhat,
xlab = "actual",
ylab = "predicted")
abline(lm(yhat ~ y))
abline(coef = c(0,1))
#plot first 3 trees of model
# xgb.plot.tree(model = m1_xgb, trees = 0:2)
importance_matrix <- xgb.importance(model = m1_xgb)
xgb.plot.importance(importance_matrix[1:10,], xlab = "Feature Importance")
postResample(exp(yhat), exp(y))
plot(exp(y),
exp(yhat),
xlab = "actual",
ylab = "predicted")
abline(lm(exp(yhat) ~ exp(y)))
abline(coef = c(0,1))
source('~/football-agent-services/football_stats/FW.R')
options(device = "RStudioGD")
importance_matrix <- xgb.importance(model = m1_xgb)
xgb.plot.importance(importance_matrix[1:10,], xlab = "Feature Importance")
postResample(exp(yhat), exp(y))
plot(exp(y),
exp(yhat),
xlab = "actual",
ylab = "predicted")
abline(lm(exp(yhat) ~ exp(y)))
abline(coef = c(0,1))
